<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Avatar MVP</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>
    <style>
        * {
            box-sizing: border-box;
        }
        
        body {
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(45deg, #1e3c72, #2a5298);
            color: #fff;
            overflow: hidden;
            -webkit-text-size-adjust: 100%;
            touch-action: manipulation;
            height: 100vh;
            display: flex;
            flex-direction: column;
        }
        
        #root {
            flex: 1;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 120px 20px 160px 20px;
        }
        
        .avatar {
            width: min(400px, 60vw);
            height: min(400px, 60vw);
            border-radius: 20px;
            background: rgba(255,255,255,0.1);
            display: flex;
            justify-content: center;
            align-items: center;
            transition: all 0.3s ease;
            box-shadow: 0 20px 40px rgba(0,0,0,0.3);
            position: relative;
            overflow: hidden;
            backdrop-filter: blur(10px);
            border: 2px solid rgba(255,255,255,0.2);
        }
        
        #avatar3d {
            width: 100%;
            height: 100%;
            display: block;
        }
        
        .avatar.speaking {
            /* No animation - 3D model handles talking animation */
        }
        .avatar.processing {
            /* No background change - using 3D model */
        }
        .avatar.active {
            /* No background change - using 3D model */
        }
        
        @keyframes float {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-20px); }
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            100% { transform: scale(1.1); }
        }
        
        .ui-overlay {
            position: fixed;
            top: 80px;
            left: 50%;
            transform: translateX(-50%);
            z-index: 100;
            display: flex;
            flex-direction: row;
            gap: 15px;
            width: calc(100% - 40px);
            max-width: 400px;
            justify-content: center;
        }
        
        button {
            padding: 16px 32px;
            background: #007bff;
            color: white;
            border: none;
            border-radius: 12px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            min-height: 52px;
            min-width: 140px;
            touch-action: manipulation;
            user-select: none;
            transition: all 0.2s ease;
            box-shadow: 0 4px 12px rgba(0,123,255,0.3);
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        button:active {
            transform: scale(0.98);
            background: #0056b3;
        }
        
        button:hover {
            background: #0056b3;
            box-shadow: 0 6px 16px rgba(0,123,255,0.4);
        }
        
        button:disabled {
            background: #6c757d;
            cursor: not-allowed;
            opacity: 0.6;
            transform: none;
        }
        
        .chat-input {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            width: calc(100% - 40px);
            max-width: 600px;
        }
        
        input {
            flex: 1;
            padding: 18px 20px;
            border: 2px solid rgba(255,255,255,0.2);
            border-radius: 12px;
            font-size: 18px;
            background: rgba(255,255,255,0.1);
            color: white;
            backdrop-filter: blur(10px);
            min-height: 56px;
        }
        
        input::placeholder {
            color: rgba(255,255,255,0.7);
        }
        
        input:focus {
            outline: none;
            border-color: #007bff;
            background: rgba(255,255,255,0.15);
        }
        
        .send-btn {
            min-width: 80px;
            flex-shrink: 0;
        }
        
        .status {
            position: fixed;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.8);
            padding: 15px 20px;
            border-radius: 12px;
            min-width: 160px;
            font-size: 16px;
            font-weight: 500;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255,255,255,0.1);
            text-align: center;
        }
        
        .response-display {
            position: fixed;
            bottom: 180px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.9);
            padding: 20px 25px;
            border-radius: 16px;
            max-width: calc(100% - 40px);
            width: auto;
            max-width: 600px;
            display: none;
            font-size: 18px;
            line-height: 1.5;
            backdrop-filter: blur(15px);
            border: 1px solid rgba(255,255,255,0.1);
            box-shadow: 0 8px 24px rgba(0,0,0,0.4);
        }
        
        /* Mobile specific adjustments */
        @media (max-width: 768px) {
            #root {
                padding: 120px 20px 180px 20px;
            }
            
            .ui-overlay {
                top: 80px;
                gap: 12px;
                flex-direction: column;
                max-width: 300px;
            }
            
            button {
                padding: 14px 28px;
                font-size: 15px;
                min-height: 48px;
                width: 100%;
                min-width: 200px;
            }
            
            .chat-input {
                bottom: 15px;
                gap: 12px;
                width: calc(100% - 30px);
            }
            
            input {
                padding: 16px 18px;
                font-size: 16px;
                min-height: 52px;
            }
            
            .send-btn {
                min-width: 90px;
                font-size: 15px;
                padding: 14px 20px;
            }
            
            .status {
                top: 15px;
                padding: 12px 16px;
                font-size: 14px;
                min-width: 140px;
            }
            
            .response-display {
                bottom: 200px;
                padding: 18px 20px;
                font-size: 16px;
                max-width: calc(100% - 30px);
            }
            
            .avatar {
                width: min(300px, 50vw);
                height: min(300px, 50vw);
            }
        }
        
        /* Small mobile devices */
        @media (max-width: 480px) {
            #root {
                padding: 140px 20px 220px 20px;
            }
            
            .ui-overlay {
                top: 80px;
                max-width: 280px;
            }
            
            .chat-input {
                flex-direction: column;
                gap: 10px;
                bottom: 10px;
            }
            
            .send-btn {
                min-width: 100%;
                margin: 0;
                padding: 14px 20px;
                font-size: 15px;
            }
            
            .response-display {
                bottom: 240px;
            }
            
            .avatar {
                width: min(250px, 45vw);
                height: min(250px, 45vw);
            }
        }
        
        /* Landscape mobile */
        @media (max-height: 600px) and (orientation: landscape) {
            #root {
                padding: 80px 20px 80px 20px;
            }
            
            .avatar {
                width: min(200px, 35vh);
                height: min(200px, 35vh);
            }
            
            .ui-overlay {
                top: 50px;
                flex-direction: row;
                gap: 10px;
                max-width: 350px;
            }
            
            button {
                width: auto;
                min-width: 120px;
                padding: 12px 24px;
                font-size: 14px;
                min-height: 44px;
            }
            
            .response-display {
                bottom: 120px;
                font-size: 14px;
                padding: 12px 16px;
            }
            
            .chat-input {
                bottom: 10px;
            }
        }
    </style>
</head>
<body>
    <div id="root">
        <div class="avatar" id="avatar">
            <canvas id="avatar3d"></canvas>
        </div>
    </div>
    
    <div class="ui-overlay">
        <button id="startBtn">Start Chat</button>
        <button id="voiceBtn">ðŸŽ¤ Voice</button>
    </div>
    
    <div class="chat-input">
        <input type="text" id="messageInput" placeholder="Type your message..." />
        <button id="sendBtn">Send</button>
    </div>
    
    <div class="status" id="status">Ready</div>
    <div class="response-display" id="responseDisplay"></div>
    
    <script>
        const avatar = document.getElementById('avatar');
        const status = document.getElementById('status');
        const responseDisplay = document.getElementById('responseDisplay');
        const messageInput = document.getElementById('messageInput');
        
        let isActive = true;  // Start in active mode by default
        let isListening = false;
        
        // Add Enter key support for immediate typing
        messageInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                document.getElementById('sendBtn').click();
            }
        });
        
        // Focus on input field when page loads
        window.addEventListener('load', () => {
            setTimeout(() => {
                messageInput.focus();
            }, 500);
        });
        
        // 3D Avatar System
        let scene, camera, renderer, avatarModel, mixer, clock;
        let mouseX = 0, mouseY = 0;
        let targetRotationX = 0, targetRotationY = 0;
        let isModelLoaded = false;
        
        // Initialize 3D Avatar
        function init3DAvatar() {
            const canvas = document.getElementById('avatar3d');
            const container = document.getElementById('avatar');
            
            // Scene setup
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(50, 1, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ 
                canvas: canvas, 
                alpha: true, 
                antialias: true 
            });
            
            const rect = container.getBoundingClientRect();
            renderer.setSize(rect.width, rect.height);
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.outputEncoding = THREE.sRGBEncoding;
            renderer.shadowMap.enabled = true;
            renderer.shadowMap.type = THREE.PCFSoftShadowMap;
            
            // Lighting
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
            scene.add(ambientLight);
            
            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
            directionalLight.position.set(1, 1, 1);
            directionalLight.castShadow = true;
            scene.add(directionalLight);
            
            const fillLight = new THREE.DirectionalLight(0x4a90e2, 0.3);
            fillLight.position.set(-1, 0, 1);
            scene.add(fillLight);
            
            // Camera position - positioned to show head and face
            camera.position.set(0, 3.0, 2.0);
            camera.lookAt(0, 3.0, 0);
            
            // Load 3D model
            const loader = new THREE.GLTFLoader();
            loader.load(
                'https://models.readyplayer.me/6864cd2efe9030c714ea760d.glb',
                function(gltf) {
                    avatarModel = gltf.scene;
                    
                    // Scale and position model to show head/face
                    avatarModel.scale.set(3.0, 3.0, 3.0);
                    avatarModel.position.set(0, -2.0, 0);
                    
                    // Enable shadows
                    avatarModel.traverse(function(child) {
                        if (child.isMesh) {
                            child.castShadow = true;
                            child.receiveShadow = true;
                            
                        }
                    });
                    
                    scene.add(avatarModel);
                    
                    // Animation mixer
                    if (gltf.animations && gltf.animations.length > 0) {
                        mixer = new THREE.AnimationMixer(avatarModel);
                        gltf.animations.forEach(clip => {
                            mixer.clipAction(clip);
                        });
                    }
                    
                    isModelLoaded = true;
                    findBones();
                },
                undefined,
                function(error) {
                    createFallbackAvatar();
                }
            );
            
            clock = new THREE.Clock();
            
            // Mouse tracking disabled
            document.addEventListener('mousemove', onMouseMove);
            window.addEventListener('resize', onWindowResize);
            
            // Start animation loop
            animate();
        }
        
        function createFallbackAvatar() {
            const geometry = new THREE.SphereGeometry(0.3, 32, 32);
            const material = new THREE.MeshPhongMaterial({ 
                color: 0x4a90e2,
                transparent: true,
                opacity: 0.8
            });
            avatarModel = new THREE.Mesh(geometry, material);
            avatarModel.position.set(0, 0, 0);
            scene.add(avatarModel);
            isModelLoaded = true;
        }
        
        function onMouseMove(event) {
            if (!isModelLoaded) return;

            console.log("mouse moved")
            
            const rect = avatar.getBoundingClientRect();
            mouseX = ((event.clientX - rect.left) / rect.width) * 2 - 1;
            mouseY = -((event.clientY - rect.top) / rect.height) * 2 + 1;
            
            targetRotationY = mouseX * 0.3;
            targetRotationX = mouseY * 0.2;
        }
        
        function onWindowResize() {
            const container = document.getElementById('avatar');
            const rect = container.getBoundingClientRect();
            
            camera.aspect = rect.width / rect.height;
            camera.updateProjectionMatrix();
            renderer.setSize(rect.width, rect.height);
        }
        
        function animate() {
            requestAnimationFrame(animate);
            
            if (isModelLoaded && avatarModel) {
                // Idle breathing animation (only when not talking)
                if (!isTalking) {
                    const time = clock.getElapsedTime();
                    avatarModel.position.y = -2.0 + Math.sin(time * 2) * 0.02;
                }
                
                // Update animations
                if (mixer) {
                    mixer.update(clock.getDelta());
                }
            }
            
            renderer.render(scene, camera);
        }
        
        let isTalking = false;
        let talkingAnimation = null;
        let headBone = null;
        let jawBone = null;
        let faceMesh = null;
        let smileBlendShape = null;
        let mouthShapes = {};
        let currentAudio = null;
        let isPlaying = false;
        
        // AWS Polly viseme mapping - neutral expressions during speech
        const visemeToMouthShape = {
            // Silence
            'sil': { open: 0.0, smile: 0.0 },
            
            // Consonants - all neutral, no smiles during speech
            'p': { open: 0.0, smile: 0.0 },     // P, B, M - lips closed
            'f': { open: 0.15, smile: 0.0 },    // F, V - lips slightly open
            't': { open: 0.3, smile: 0.0 },     // T, D, N, L - tongue tip
            'S': { open: 0.2, smile: 0.0 },     // S, Z - neutral
            's': { open: 0.2, smile: 0.0 },     // S, Z - neutral
            'r': { open: 0.3, smile: 0.0 },     // R - rounded
            'k': { open: 0.25, smile: 0.0 },    // K, G - neutral
            
            // Vowels - natural mouth shapes without smiles
            'a': { open: 0.6, smile: 0.0 },     // AH, AA - moderately wide
            'e': { open: 0.4, smile: 0.0 },     // EH, EY - medium open
            'i': { open: 0.2, smile: 0.0 },     // IH, IY - small opening
            'o': { open: 0.5, smile: 0.0 },     // OH, OW - round
            'u': { open: 0.3, smile: 0.0 },     // UH, UW - small round
            '@': { open: 0.4, smile: 0.0 },     // Schwa - neutral
            
            // Uppercase versions - all neutral
            'PP': { open: 0.0, smile: 0.0 },
            'FF': { open: 0.15, smile: 0.0 },
            'TH': { open: 0.25, smile: 0.0 },
            'DD': { open: 0.3, smile: 0.0 },
            'kk': { open: 0.25, smile: 0.0 },
            'CH': { open: 0.2, smile: 0.0 },
            'SS': { open: 0.2, smile: 0.0 },
            'nn': { open: 0.2, smile: 0.0 },
            'RR': { open: 0.3, smile: 0.0 },
            'aa': { open: 0.6, smile: 0.0 },
            'E': { open: 0.4, smile: 0.0 },
            'I': { open: 0.2, smile: 0.0 },
            'O': { open: 0.5, smile: 0.0 },
            'U': { open: 0.3, smile: 0.0 },
            'T': { open: 0.3, smile: 0.0 }
        };
        
        function setAvatarState(state) {
            avatar.className = 'avatar';
            
            if (!isModelLoaded) return;
            
            // Find facial features if not already found
            if (!faceMesh) {
                findBones();
            }
            
            switch(state) {
                case 'active':
                    avatar.className = 'avatar active';
                    stopTalking();
                    setSmile(0.6); // Happy smile when active
                    break;
                case 'processing':
                    avatar.className = 'avatar processing';
                    stopTalking();
                    clearSmile(); // Neutral while thinking
                    break;
                case 'speaking':
                    avatar.className = 'avatar speaking';
                    startTalking();
                    clearSmile(); // Clear any existing smile from previous states
                    break;
                case 'listening':
                    avatar.className = 'avatar processing';
                    stopTalking();
                    setSmile(0.4); // Attentive smile while listening
                    break;
                default:
                    stopTalking();
                    clearSmile();
                    break;
            }
        }
        
        function findBones() {
            if (!avatarModel) return;
            
            // Try to find head, jaw bones and facial blend shapes
            avatarModel.traverse(function(child) {
                // Look for bones
                if (child.isBone || child.name) {
                    const name = child.name.toLowerCase();
                    if (name.includes('head') && !headBone) {
                        headBone = child;
                    }
                    if (name.includes('jaw') && !jawBone) {
                        jawBone = child;
                    }
                }
                
                // Look for facial mesh with morph targets
                if (child.isMesh && child.morphTargetDictionary) {
                    faceMesh = child;
                    const morphTargets = child.morphTargetDictionary;
                    
                    // Look for smile-related blend shapes
                    for (let target in morphTargets) {
                        const targetName = target.toLowerCase();
                        if (targetName.includes('smile') || targetName.includes('happy') || targetName.includes('joy')) {
                            smileBlendShape = { mesh: child, index: morphTargets[target] };
                            break;
                        }
                    }
                    
                    // Look for mouth shapes (basic or ARKit visemes)
                    const hasMouthShapes = morphTargets.hasOwnProperty('mouthOpen') || 
                                         morphTargets.hasOwnProperty('mouthSmile') ||
                                         morphTargets.hasOwnProperty('viseme_sil') ||
                                         morphTargets.hasOwnProperty('mouthFunnel') ||
                                         Object.keys(morphTargets).some(key => 
                                            key.toLowerCase().includes('mouth') || 
                                            key.toLowerCase().includes('viseme') ||
                                            key.toLowerCase().includes('jaw')
                                         );
                    
                    if (hasMouthShapes) {
                        // Exclude eye meshes from mouth animation
                        const meshName = child.name || '';
                        const isEyeMesh = meshName.toLowerCase().includes('eye');
                        
                        if (!isEyeMesh) {
                            mouthShapes[child.uuid] = {
                                mesh: child,
                                morphTargets: morphTargets
                            };
                        }
                    }
                }
            });
            
        }
        
        // ARKit viseme mapping - NO SMILES during speech
        const arkitVisemeMapping = {
            'sil': { shapes: ['viseme_sil', 'mouthClose'], weight: 1.0 },
            'p': { shapes: ['viseme_PP', 'mouthClose'], weight: 1.0 },
            'f': { shapes: ['viseme_FF', 'mouthOpen'], weight: 0.3 },
            't': { shapes: ['viseme_DD', 'mouthOpen'], weight: 0.6 },
            's': { shapes: ['viseme_SS', 'mouthOpen'], weight: 0.4 },
            'S': { shapes: ['viseme_SS', 'mouthOpen'], weight: 0.4 },
            'r': { shapes: ['viseme_RR', 'mouthOpen'], weight: 0.5 },
            'k': { shapes: ['viseme_kk', 'mouthOpen'], weight: 0.4 },
            'a': { shapes: ['viseme_aa', 'mouthOpen'], weight: 1.0 },
            'e': { shapes: ['viseme_E', 'mouthOpen'], weight: 0.7 },
            'i': { shapes: ['viseme_I', 'mouthOpen'], weight: 0.8 },
            'o': { shapes: ['viseme_O', 'mouthOpen'], weight: 0.9 },
            'u': { shapes: ['viseme_U', 'mouthOpen'], weight: 0.8 },
            '@': { shapes: ['viseme_aa', 'mouthOpen'], weight: 0.6 },
            'E': { shapes: ['viseme_E', 'mouthOpen'], weight: 0.7 },
            'O': { shapes: ['viseme_O', 'mouthOpen'], weight: 0.9 },
            'T': { shapes: ['viseme_DD', 'mouthOpen'], weight: 0.6 }
        };

        // Set viseme using advanced ARKit shapes or fallback to basic shapes
        function setViseme(viseme, weight = 1.0) {
            if (!avatarModel) return;
            if (Object.keys(mouthShapes).length === 0) return;
            
            let shapesApplied = 0;
            
            // Try ARKit mapping first
            const arkitMapping = arkitVisemeMapping[viseme];
            if (arkitMapping) {
                Object.values(mouthShapes).forEach(shapeData => {
                    const morphTargets = shapeData.morphTargets;
                    const mesh = shapeData.mesh;
                    
                    // Reset all mouth-related shapes first
                    Object.keys(morphTargets).forEach(targetName => {
                        if (targetName.toLowerCase().includes('mouth') || 
                            targetName.toLowerCase().includes('viseme') ||
                            targetName.toLowerCase().includes('jaw')) {
                            mesh.morphTargetInfluences[morphTargets[targetName]] = 0;
                        }
                    });
                    
                    // Apply the specific shapes for this viseme
                    arkitMapping.shapes.forEach(shapeName => {
                        if (morphTargets.hasOwnProperty(shapeName)) {
                            const shapeIndex = morphTargets[shapeName];
                            const shapeWeight = arkitMapping.weight * weight;
                            mesh.morphTargetInfluences[shapeIndex] = shapeWeight;
                            shapesApplied++;
                        }
                    });
                });
            }
            
            // Fallback to basic mouth shapes if no ARKit shapes found
            if (shapesApplied === 0) {
                const mouthShape = visemeToMouthShape[viseme];
                if (mouthShape) {
                    Object.values(mouthShapes).forEach(shapeData => {
                        const morphTargets = shapeData.morphTargets;
                        const mesh = shapeData.mesh;
                        
                        // Set mouthOpen with clamping to prevent negative values
                        if (morphTargets.hasOwnProperty('mouthOpen')) {
                            const openIndex = morphTargets['mouthOpen'];
                            const openValue = Math.max(0, Math.min(1, mouthShape.open * weight));
                            mesh.morphTargetInfluences[openIndex] = openValue;
                            shapesApplied++;
                        }
                        
                        // NO SMILES during speech - keep mouthSmile at 0 during lip sync
                        if (morphTargets.hasOwnProperty('mouthSmile')) {
                            const smileIndex = morphTargets['mouthSmile'];
                            mesh.morphTargetInfluences[smileIndex] = 0.0; // Always 0 during speech
                            shapesApplied++;
                        }
                    });
                }
            }
        }
        
        // Play browser TTS with lip sync animation
        async function playBrowserTTSWithLipSync(text, visemes) {
            return new Promise((resolve, reject) => {
                if (!('speechSynthesis' in window)) {
                    reject(new Error('Speech synthesis not supported'));
                    return;
                }
                
                const utterance = new SpeechSynthesisUtterance(text);
                
                // Set male voice
                const voices = speechSynthesis.getVoices();
                const maleVoice = voices.find(voice => 
                    voice.name.toLowerCase().includes('eddy') ||
                    voice.name.toLowerCase().includes('fred') ||
                    voice.name.toLowerCase().includes('daniel') ||
                    voice.name.toLowerCase().includes('david') ||
                    voice.name.toLowerCase().includes('mark') ||
                    (voice.lang.startsWith('en') && voice.gender === 'male')
                );
                
                if (maleVoice) {
                    utterance.voice = maleVoice;
                }
                
                utterance.rate = 0.85;
                utterance.pitch = 0.6;
                utterance.volume = 0.8;
                
                let visemeIndex = 0;
                let animationId = null;
                let startTime = null;
                let isPlaying = true;
                
                const animateLipSync = () => {
                    if (!isPlaying) return;
                    
                    const elapsed = startTime ? (Date.now() - startTime) / 1000 : 0;
                    
                    // Find the current viseme based on elapsed time
                    let targetIndex = visemeIndex;
                    for (let i = 0; i < visemes.length; i++) {
                        if (visemes[i].time <= elapsed) {
                            targetIndex = i;
                        } else {
                            break;
                        }
                    }
                    
                    // Update viseme if changed
                    if (targetIndex !== visemeIndex) {
                        visemeIndex = targetIndex;
                        const currentViseme = visemes[visemeIndex];
                        setViseme(currentViseme.viseme);
                    }
                    
                    // Continue animation
                    if (isPlaying) {
                        animationId = requestAnimationFrame(animateLipSync);
                    }
                };
                
                utterance.onstart = () => {
                    startTime = Date.now();
                    animationId = requestAnimationFrame(animateLipSync);
                };
                
                utterance.onend = () => {
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                    setViseme('sil');
                    
                    // Add post-speech smile
                    setTimeout(() => {
                        addPostSpeechSmile();
                    }, 200);
                    
                    resolve();
                };
                
                utterance.onerror = (error) => {
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                    setViseme('sil');
                    reject(error);
                };
                
                speechSynthesis.speak(utterance);
            });
        }
        
        // Play audio with lip sync animation
        async function playAudioWithLipSync(audioDataUrl, visemes) {
            return new Promise((resolve, reject) => {
                // Create audio element
                currentAudio = new Audio(audioDataUrl);
                isPlaying = true;
                
                let visemeIndex = 0;
                let animationId = null;
                let audioStartTime = null;
                
                const animateLipSync = () => {
                    if (!isPlaying || !currentAudio) return;
                    
                    // Use audio's currentTime for more accurate sync
                    const elapsed = currentAudio.currentTime;
                    
                    // Find the current viseme based on audio time
                    let targetIndex = visemeIndex;
                    
                    // Look ahead to find the right viseme for current time
                    for (let i = 0; i < visemes.length; i++) {
                        if (visemes[i].time <= elapsed) {
                            targetIndex = i;
                        } else {
                            break;
                        }
                    }
                    
                    // Only update if we've moved to a new viseme
                    if (targetIndex !== visemeIndex) {
                        visemeIndex = targetIndex;
                        const currentViseme = visemes[visemeIndex];
                        setViseme(currentViseme.viseme);
                    }
                    
                    // Continue animation while audio is playing
                    if (!currentAudio.paused && !currentAudio.ended) {
                        animationId = requestAnimationFrame(animateLipSync);
                    } else {
                        isPlaying = false;
                        setViseme('sil');
                    }
                };
                
                currentAudio.onplay = () => {
                    audioStartTime = Date.now();
                    animationId = requestAnimationFrame(animateLipSync);
                };
                
                currentAudio.onended = () => {
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                    setViseme('sil');
                    
                    // Add a smile after finishing speech
                    setTimeout(() => {
                        addPostSpeechSmile();
                    }, 200);
                    
                    resolve();
                };
                
                currentAudio.onpause = () => {
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                };
                
                currentAudio.onerror = (error) => {
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                    setViseme('sil');
                    reject(error);
                };
                
                currentAudio.play().catch(reject);
            });
        }
        
        // Add a pleasant smile after finishing speech
        function addPostSpeechSmile() {
            if (!avatarModel || Object.keys(mouthShapes).length === 0) return;
            
            // Apply a gentle smile to all meshes
            Object.values(mouthShapes).forEach(shapeData => {
                const morphTargets = shapeData.morphTargets;
                const mesh = shapeData.mesh;
                
                // Reset mouth open to neutral
                if (morphTargets.hasOwnProperty('mouthOpen')) {
                    const openIndex = morphTargets['mouthOpen'];
                    mesh.morphTargetInfluences[openIndex] = 0.0;
                }
                
                // Set a pleasant smile
                if (morphTargets.hasOwnProperty('mouthSmile')) {
                    const smileIndex = morphTargets['mouthSmile'];
                    mesh.morphTargetInfluences[smileIndex] = 0.4;
                }
            });
            
            // Remove smile after a few seconds
            setTimeout(() => {
                Object.values(mouthShapes).forEach(shapeData => {
                    const morphTargets = shapeData.morphTargets;
                    const mesh = shapeData.mesh;
                    
                    if (morphTargets.hasOwnProperty('mouthSmile')) {
                        const smileIndex = morphTargets['mouthSmile'];
                        mesh.morphTargetInfluences[smileIndex] = 0.0;
                    }
                });
            }, 2500);
        }
        
        function setSmile(intensity = 0.5) {
            if (smileBlendShape && smileBlendShape.mesh.morphTargetInfluences) {
                smileBlendShape.mesh.morphTargetInfluences[smileBlendShape.index] = Math.max(0, Math.min(1, intensity));
            }
        }
        
        function clearSmile() {
            // Clear the original smile blend shape
            if (smileBlendShape && smileBlendShape.mesh.morphTargetInfluences) {
                smileBlendShape.mesh.morphTargetInfluences[smileBlendShape.index] = 0;
            }
            
            // Also clear mouthSmile on all mouth shape meshes
            Object.values(mouthShapes).forEach(shapeData => {
                const morphTargets = shapeData.morphTargets;
                const mesh = shapeData.mesh;
                
                if (morphTargets.hasOwnProperty('mouthSmile')) {
                    const smileIndex = morphTargets['mouthSmile'];
                    mesh.morphTargetInfluences[smileIndex] = 0.0;
                }
            });
        }
        
        function startTalking() {
            if (isTalking) return;
            isTalking = true;
            
            // Simple talking animation - only head rotation, no position changes
            function talkingLoop() {
                if (!isTalking || !avatarModel) return;
                
                const time = performance.now() * 0.003;
                
                // Continuous gentle head bobbing - rotation only
                avatarModel.rotation.y = Math.sin(time) * 0.015;
                avatarModel.rotation.x = Math.sin(time * 0.8) * 0.01;
                avatarModel.rotation.z = Math.sin(time * 1.2) * 0.005;
                
                // NO position changes - keep avatar container stable
                
                talkingAnimation = requestAnimationFrame(talkingLoop);
            }
            
            talkingLoop();
        }
        
        function stopTalking() {
            isTalking = false;
            
            if (talkingAnimation) {
                cancelAnimationFrame(talkingAnimation);
                talkingAnimation = null;
            }
            
            // Reset to neutral position smoothly - no position changes
            if (avatarModel) {
                avatarModel.rotation.x = 0;
                avatarModel.rotation.y = 0;
                avatarModel.rotation.z = 0;
            }
        }
        
        // Test function for manual viseme testing (call from console)
        window.testViseme = function(viseme = 'aa') {
            setViseme(viseme, 1.0);
            setTimeout(() => {
                setViseme('sil', 0);
            }, 2000);
        };
        
        // Test function to show all available blend shapes
        window.showBlendShapes = function() {
            if (!avatarModel) {
                return;
            }
            
            avatarModel.traverse(function(child) {
                if (child.isMesh && child.morphTargetDictionary) {
                    // Debug function for checking available blend shapes
                }
            });
        };
        
        // Test ONLY mouth opening (no smile)
        window.testMouthOpen = function(amount = 0.5) {
            if (!avatarModel || Object.keys(mouthShapes).length === 0) return;
            
            Object.values(mouthShapes).forEach(shapeData => {
                const morphTargets = shapeData.morphTargets;
                const mesh = shapeData.mesh;
                
                if (morphTargets.hasOwnProperty('mouthOpen')) {
                    const openIndex = morphTargets['mouthOpen'];
                    mesh.morphTargetInfluences[openIndex] = amount;
                }
                
                if (morphTargets.hasOwnProperty('mouthSmile')) {
                    const smileIndex = morphTargets['mouthSmile'];
                    mesh.morphTargetInfluences[smileIndex] = 0.0;
                }
            });
        };
        
        // Test ONLY smile (no mouth open)
        window.testSmileOnly = function(amount = 0.5) {
            if (!avatarModel || Object.keys(mouthShapes).length === 0) return;
            
            Object.values(mouthShapes).forEach(shapeData => {
                const morphTargets = shapeData.morphTargets;
                const mesh = shapeData.mesh;
                
                if (morphTargets.hasOwnProperty('mouthOpen')) {
                    const openIndex = morphTargets['mouthOpen'];
                    mesh.morphTargetInfluences[openIndex] = 0.0;
                }
                
                if (morphTargets.hasOwnProperty('mouthSmile')) {
                    const smileIndex = morphTargets['mouthSmile'];
                    mesh.morphTargetInfluences[smileIndex] = amount;
                }
            });
        };
        
        // Reset mouth to neutral
        window.resetMouth = function() {
            if (!avatarModel || Object.keys(mouthShapes).length === 0) {
                return;
            }
            
            Object.values(mouthShapes).forEach(shapeData => {
                const morphTargets = shapeData.morphTargets;
                const mesh = shapeData.mesh;
                
                if (morphTargets.hasOwnProperty('mouthOpen')) {
                    const openIndex = morphTargets['mouthOpen'];
                    mesh.morphTargetInfluences[openIndex] = 0.0;
                }
                
                if (morphTargets.hasOwnProperty('mouthSmile')) {
                    const smileIndex = morphTargets['mouthSmile'];
                    mesh.morphTargetInfluences[smileIndex] = 0.0;
                }
            });
        };

        // Initialize 3D avatar when page loads
        window.addEventListener('load', init3DAvatar);
        
        // Initialize in active mode when page loads
        window.addEventListener('load', () => {
            setTimeout(() => {
                status.textContent = 'Ready - Type your message';
                setAvatarState('idle');
            }, 1000);
        });
        
        // Start chat (used to return from voice mode)
        document.getElementById('startBtn').addEventListener('click', () => {
            isActive = true;
            status.textContent = 'Chat Active';
            setAvatarState('active');
            
            // No automatic speech - just activate quietly
            
            setTimeout(() => {
                setAvatarState('idle');
            }, 2000);
        });
        
        // Send message
        document.getElementById('sendBtn').addEventListener('click', async () => {
            const message = messageInput.value.trim();
            
            if (!message) return;
            
            if (!isActive) {
                // Auto-activate if not active
                isActive = true;
                status.textContent = 'Chat Active';
            }
            
            // Show processing
            status.textContent = 'Processing...';
            setAvatarState('processing');
            messageInput.value = '';
            responseDisplay.style.display = 'none';
            
            try {
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ message })
                });
                
                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }
                
                const data = await response.json();
                const aiResponse = data.response;
                
                // Show response
                responseDisplay.textContent = aiResponse;
                responseDisplay.style.display = 'block';
                
                // Try to generate speech with AWS Polly (male voice)
                try {
                    status.textContent = 'Generating speech...';
                    console.log('ðŸŽ¤ Requesting speech for:', aiResponse);
                    
                    const audioResponse = await fetch('/api/speak', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({ 
                            text: aiResponse,
                            voice: 'Brian',          // Try Brian (British), Joey, or Russell
                            voiceId: 'Matthew',      // Alternative parameter name
                            VoiceId: 'Matthew',      // AWS API format
                            gender: 'male',
                            // Try multiple male voices
                            Engine: 'standard'
                        })
                    });
                    
                    if (audioResponse.ok) {
                        const contentType = audioResponse.headers.get('content-type');
                        console.log('ðŸ“¡ Audio response content-type:', contentType);
                        
                        // Check if it's JSON response (new format with visemes)
                        if (contentType && contentType.includes('application/json')) {
                            const audioData = await audioResponse.json();
                            console.log('ðŸ“Š Audio data received:', audioData);
                            
                            // Check if we have viseme data for lip sync
                            if (audioData.audio && audioData.visemes) {
                                console.log('ðŸŽ¯ AWS Polly with visemes! Count:', audioData.visemes.length);
                                status.textContent = 'Speaking with lip sync...';
                                setAvatarState('speaking');
                                
                                try {
                                    // Play audio with lip sync
                                    await playAudioWithLipSync(audioData.audio, audioData.visemes);
                                    
                                    status.textContent = 'Ready';
                                    setAvatarState('idle');
                                    
                                    setTimeout(() => {
                                        responseDisplay.style.display = 'none';
                                    }, 3000);
                                    
                                } catch (error) {
                                    console.error('Lip sync playback failed:', error);
                                    fallbackToTextDisplay();
                                }
                                
                            } else if (audioData.useBrowserTTS) {
                                // Handle browser TTS with or without visemes
                                if (audioData.visemes && audioData.visemes.length > 0) {
                                    console.log('ðŸŽ¯ Browser TTS with generated visemes! Count:', audioData.visemes.length);
                                    status.textContent = 'Speaking with lip sync...';
                                    setAvatarState('speaking');
                                    
                                    // Play browser TTS with lip sync
                                    await playBrowserTTSWithLipSync(audioData.text, audioData.visemes);
                                } else {
                                    console.log('ðŸ”Š Using browser TTS fallback (no visemes)');
                                    status.textContent = 'Speaking...';
                                    setAvatarState('speaking');
                                }
                                
                                if ('speechSynthesis' in window && (!audioData.visemes || audioData.visemes.length === 0)) {
                                    const utterance = new SpeechSynthesisUtterance(audioData.text);
                                    
                                    // Set male voice - prioritize Eddy on macOS
                                    const voices = speechSynthesis.getVoices();
                                    const maleVoice = voices.find(voice => 
                                        voice.name.toLowerCase().includes('eddy') ||         // Preferred male voice on Mac
                                        voice.name.toLowerCase().includes('fred') ||        // Deep male voice on Mac
                                        voice.name.toLowerCase().includes('daniel') ||      // Male voice on Mac  
                                        voice.name.toLowerCase().includes('david') ||       // Windows male
                                        voice.name.toLowerCase().includes('mark') ||        // Windows male
                                        voice.name.toLowerCase().includes('richard') ||     // Windows male
                                        voice.name.toLowerCase().includes('google uk english male') ||
                                        voice.name.toLowerCase().includes('google us english male') ||
                                        (voice.lang.startsWith('en') && voice.gender === 'male')
                                    );
                                    
                                    if (maleVoice) {
                                        utterance.voice = maleVoice;
                                    } else {
                                    }
                                    
                                    utterance.rate = 0.85; // Slightly slower for deeper sound
                                    utterance.pitch = 0.6; // Much lower pitch for masculine sound
                                    utterance.volume = 0.8;
                                    
                                    utterance.onend = () => {
                                        status.textContent = 'Ready';
                                        setAvatarState('idle');
                                        
                                        setTimeout(() => {
                                            responseDisplay.style.display = 'none';
                                        }, 3000);
                                    };
                                    
                                    utterance.onerror = () => {
                                        console.error('Browser TTS failed');
                                        fallbackToTextDisplay();
                                    };
                                    
                                    speechSynthesis.speak(utterance);
                                } else {
                                    fallbackToTextDisplay();
                                }
                            } else {
                                fallbackToTextDisplay();
                            }
                            
                        } else if (contentType && contentType.includes('audio')) {
                            // Legacy audio format (no visemes)
                            const audioBlob = await audioResponse.blob();
                            const audioUrl = URL.createObjectURL(audioBlob);
                            const audio = new Audio(audioUrl);
                            
                            status.textContent = 'Speaking...';
                            setAvatarState('speaking');
                            
                            audio.play().catch(error => {
                                console.error('Audio play failed:', error);
                                fallbackToTextDisplay();
                            });
                            
                            audio.onended = () => {
                                status.textContent = 'Ready';
                                setAvatarState('idle');
                                URL.revokeObjectURL(audioUrl);
                                
                                setTimeout(() => {
                                    responseDisplay.style.display = 'none';
                                }, 3000);
                            };
                            
                            audio.onerror = () => {
                                console.error('Audio playback failed');
                                fallbackToTextDisplay();
                            };
                            
                        } else {
                            fallbackToTextDisplay();
                        }
                        
                    } else {
                        fallbackToTextDisplay();
                    }
                    
                } catch (ttsError) {
                    console.error('TTS error:', ttsError);
                    fallbackToTextDisplay();
                }
                
                function fallbackToTextDisplay() {
                    status.textContent = 'Speaking...';
                    setAvatarState('speaking');
                    
                    const speakingTime = Math.max(2000, aiResponse.length * 50);
                    setTimeout(() => {
                        status.textContent = 'Ready';
                        setAvatarState('idle');
                        
                        setTimeout(() => {
                            responseDisplay.style.display = 'none';
                        }, 3000);
                    }, speakingTime);
                }
                
            } catch (error) {
                console.error('Chat error:', error);
                status.textContent = 'Error - using fallback';
                
                // Fallback responses
                const fallbackResponses = [
                    "Hello! How can I help you today?",
                    "That's interesting! Tell me more.",
                    "I understand what you're saying.",
                    "Great question! Let me think about that.",
                    "I'm here to help with whatever you need."
                ];
                
                const response = fallbackResponses[Math.floor(Math.random() * fallbackResponses.length)];
                responseDisplay.textContent = response;
                responseDisplay.style.display = 'block';
                setAvatarState('speaking');
                
                setTimeout(() => {
                    status.textContent = 'Ready';
                    setAvatarState('idle');
                    setTimeout(() => {
                        responseDisplay.style.display = 'none';
                    }, 3000);
                }, 2000);
            }
        });
        
        // Voice input
        document.getElementById('voiceBtn').addEventListener('click', () => {
            if (!isActive) {
                // Auto-activate if not active
                isActive = true;
                status.textContent = 'Chat Active';
            }
            
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                const recognition = new SpeechRecognition();
                
                recognition.continuous = false;
                recognition.interimResults = false;
                recognition.lang = 'en-US';
                
                if (!isListening) {
                    recognition.start();
                    isListening = true;
                    document.getElementById('voiceBtn').textContent = 'ðŸ”´ Listening...';
                    status.textContent = 'Listening...';
                    setAvatarState('listening');
                }
                
                recognition.onresult = (event) => {
                    const transcript = event.results[0][0].transcript;
                    messageInput.value = transcript;
                    document.getElementById('sendBtn').click();
                };
                
                recognition.onend = () => {
                    isListening = false;
                    document.getElementById('voiceBtn').textContent = 'ðŸŽ¤ Voice';
                    if (status.textContent === 'Listening...') {
                        status.textContent = 'Ready';
                        setAvatarState('idle');
                    }
                };
                
                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    isListening = false;
                    document.getElementById('voiceBtn').textContent = 'ðŸŽ¤ Voice';
                    status.textContent = 'Speech error - try typing';
                    setAvatarState('idle');
                };
            } else {
                alert('Speech recognition not supported in this browser. Try Chrome or Edge.');
            }
        });
        
        // Enter key support
        messageInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                document.getElementById('sendBtn').click();
            }
        });
    </script>
</body>
</html>
